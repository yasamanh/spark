{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "matplotlib.rcParams['savefig.dpi'] = 144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import toolz\n",
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local[*]\", \"pyspark_df\")\n",
    "print sc.version\n",
    "\n",
    "# Alternatively...\n",
    "# conf = SparkConf().setAppName(\"pyspark_df\").setMaster(\"local[*]\")\n",
    "# sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# needed to convert RDDs into DataFrames\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import UserDefinedFunction as udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrames\n",
    "\n",
    "## Motivation and Spark SQL\n",
    "\n",
    "Spark SQL is the current effort to provide support for writing SQL queries in Spark. Newer versions support Hive, Parquet, and other data sources. [Docs](http://spark.apache.org/docs/latest/sql-programming-guide.html)\n",
    "\n",
    "The key feature of Spark SQL is the use of DataFrames instead of RDDs. A DataFrame is a distributed collection of data organized into named columns, and operations on DataFrames are first parsed through an optimized execution engine which streamlines and may even reorder the request to optimize execution. The keyword to search here is Catalyst.\n",
    "\n",
    "Under the hood, operations on DataFrames are boiled down to operations on RDDs, but the RDDs are created by the execution engine, and not directly by the user. It is also possible to convert RDDs to DataFrames and vice versa.\n",
    "\n",
    "The Spark ML package, unlike MLlib, uses DataFrames as inputs and outputs.\n",
    "\n",
    "**Question:** What is an example of a \"bad\" sequence of operations which should be reordered for optimal performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "DataFrames are...\n",
    "\n",
    "* Immutable, like RDDs\n",
    "* Lineage is remembered, like RDDs (resiliency)\n",
    "* Lazy execution, like RDDs\n",
    "* So why do we care?\n",
    "\n",
    "\n",
    "DataFrames are an abstraction that lets us think of data in a familiar form (Panda, data.frame, SQL table, etc.).\n",
    "\n",
    "We can use a similar API to RDDs!\n",
    "\n",
    "Access to SQL-like optimizations and cost analysis due to it being in a columnar format.\n",
    "\n",
    "What about type safety?\n",
    "\n",
    "What are these UDF things?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = sc.parallelize(xrange(1,10001)) \\\n",
    "         .map(lambda x: (random.random(), random.random()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = data.toDF()\n",
    "# Note: this isn't always so easy, you may need to explicitly specify a schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df.withColumnRenamed(\"_1\", \"x\").withColumnRenamed(\"_2\", \"y\")\n",
    "df.write.save(\"parquet_demo_pyspark\", format=\"parquet\")\n",
    "# Another (older) syntax\n",
    "# df.write.parquet(\"file:///home/vagrant/datacourse/module5/demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try rerunning the above cell.\n",
    "\n",
    "Save modes:\n",
    "* error\n",
    "* append\n",
    "* overwrite\n",
    "* ignore (ie. CREATE TABLE IF NOT EXISTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.write.mode(\"ignore\").parquet(\"parquet_demo_pyspark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfp = sqlContext.read.parquet(\"parquet_demo_pyspark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfp.describe(\"x\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filtered_dfp = dfp.filter(dfp[\"x\"] < 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filtered_dfp.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Catalyst Optimzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filtered_dfp.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filtered_df = df.filter(df[\"x\"] < 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filtered_df.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filtered_df = df.filter(df[\"x\"] < 0.5).filter(df[\"y\"] < 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filtered_df.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filtered_dfp = dfp.filter(dfp[\"x\"] < 0.5).filter(dfp[\"y\"] < 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filtered_dfp.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the hood, it's just manipulating trees based on rules.\n",
    "The introductory [blog post](https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html) has good pictures.\n",
    "\n",
    "### Project Tungsten\n",
    "\n",
    "* Memory management and GC (better than the JVM)\n",
    "* Cache-aware computation\n",
    "* Codegen (compile queries into Java bytecode)\n",
    "\n",
    "Cache-aware computation example:\n",
    "* Case 1: pointer -> key, value\n",
    "* Case 2: ke, pointer -> key, value\n",
    "\n",
    "The CPU has to find keys for sort purposes. This helps it find them faster.\n",
    "\n",
    "[More](https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html)\n",
    "\n",
    "### DataFrame performance and tuning\n",
    "\n",
    "See [here](http://spark.apache.org/docs/latest/sql-programming-guide.html#performance-tuning) for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL and DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Requires Hive to permanently store tables\n",
    "df.registerTempTable('nums')  # This is NOT the same as a temp table in SQL proper\n",
    "sql_df = sqlContext.sql(\"select x, y from nums where y > 0.9 limit 3\")\n",
    "sql_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sql_df.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Reminder:* Check the UI (port 4040 by default) for tables in memory.\n",
    "\n",
    "*Reminder:* A number of interactive tutorials are available on the DataBricks [community cloud](https://community.cloud.databricks.com). I highly recommend making an account and checking out the guide.\n",
    "\n",
    "This is also a good place to learn about connecting to databases like Cassandra or using JDBC protocol."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding columns and functions\n",
    "\n",
    "Because DataFrames are immutable, adding new information means appending columns to an existing DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Currying lets us specify some of a function's arguments and delay specifying the rest until later.\n",
    "\n",
    "@toolz.curry\n",
    "def prediction(threshold, val):\n",
    "    if val > threshold:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_labelizer = udf(prediction(0.5), DoubleType())\n",
    "y_labelizer = udf(prediction(0.9), DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_df = dfp.withColumn(\"x_label\", x_labelizer(\"x\")).withColumn(\"y_label\", y_labelizer(\"y\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type safety and Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd = new_df.rdd\n",
    "row = rdd.take(1)\n",
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remember that take always returns a list of results\n",
    "print type(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "row = row[0]\n",
    "print type(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Python, we're not too worried about type safety. But it's important to note that in Scala/Java, these Row objects do not contain the type information of the objects inside them and therefore type safety can be lost converting from RDDs to DataFrames. [DataSets](http://spark.apache.org/docs/latest/sql-programming-guide.html#datasets-and-dataframes) (fleshed out in Spark 2.0) are a newer incarnation of DataFrames that add encoding information to preserve that type safety.We can, however, drill into Row objects to extract the information we want.\n",
    "\n",
    "We can, however, drill into Row objects to extract the information we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "row[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "row.asDict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright &copy; 2016 The Data Incubator.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
