{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "matplotlib.rcParams['savefig.dpi'] = 144"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Creating Spark Applications\n",
    "<!-- requirement: projects/simple-spark-project -->\n",
    "<!-- requirement: projects/simple-pyspark-project -->\n",
    "<!-- requirement: small_data/gutenberg -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## REPLs\n",
    "\n",
    "Spark has a built-in **R**ead-**E**valuate-**P**rint-**L**oop in the form of a shell that you can use for interactive analysis (similar to ipython).\n",
    "\n",
    "*Scala*: `$SPARK_HOME/bin/spark-shell`\n",
    "\n",
    "*Python*: `$SPARK_HOME/bin/pyspark`\n",
    "\n",
    "You can also use Spark kernels in Jupyter notebooks.\n",
    "\n",
    "*Scala*: The Spark-Scala kernel automatically puts `val sc` in the namespace as a handle to the Spark Context. \n",
    "\n",
    "*Python*: Simply `import pyspark`, then eg.\n",
    "`sc = pyspark.SparkContext(\"local[*]\", \"demo\")`.\n",
    "\n",
    "If this doesn't work, make sure your PYTHONPATH contains the module:\n",
    "``` bash\n",
    "export PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/:$PYTHONPATH\n",
    "```\n",
    "\n",
    "*R*: Same thing:\n",
    "```\n",
    "library(SparkR, lib.loc = c(file.path(Sys.getenv(\"SPARK_HOME\"), \"R\", \"lib\")))\n",
    "\n",
    "sparkR.session(master = \"local[*]\", sparkConfig = list(spark.driver.memory = \"2g\"))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Telling Spark to look in the local file system\n",
    "import os\n",
    "def localpath(path):\n",
    "    return 'file://' + str(os.path.abspath(os.path.curdir)) + '/' + path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total lines: 77931\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.setMaster(\"local[*]\")\n",
    "conf.setAppName(\"main\")\n",
    "sc = SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "lines = sc.textFile(localpath(\"small_data/gutenberg/\"))\n",
    "totalLines = lines.count()\n",
    "print \"total lines: %d\" % totalLines\n",
    "\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Notes\n",
    "\n",
    "* You can only have one Spark Context active at a time. You'll need to stop it before starting a new one.\n",
    "* Scala (like Java) requires everything to be inside a class or object.  The repl will accept a global assignment like\n",
    "```scala\n",
    "val x = 1\n",
    "```\n",
    "but this will not work in application code that you compile.  You would need to put this into an object or class.\n",
    "\n",
    "* The PySpark API is similar to Scala, but not exactly, and there may be missing features. Be careful when looking through the [documentation](https://spark.apache.org/docs/latest/programming-guide.html).\n",
    "* PySpark comes installed with all versions of Spark, and you should be able to `import pyspark` without any trouble.\n",
    "* For machine learning applications, you'll probably need to work within a Spark SQL context as well as the usual Spark Context - this is to enable DataFrame functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Building Spark applications\n",
    "\n",
    "While the interactive console is fun, it is (likely) not how you will be submitting a job.  Instead, you will want to follow these steps.\n",
    "\n",
    "### Python\n",
    "\n",
    "It's straightforward to run a PySpark script as an app. Simply pass it to spark-submit:\n",
    "\n",
    "`spark-submit --py-files src/classes.py src/app.py arg1 arg2`\n",
    "\n",
    "In this example we pass command line arguments to main.py using `argparse` and eg. `def main(arg1, arg2):`\n",
    "\n",
    "Use the `--py-files` flag with `spark-submit` to specify additional Python modules which should be made available to each worker. These may include class definitions or third-party dependencies. Usually, if you're using classes, you will not be able to define them in the main file.\n",
    "\n",
    "If you have many python modules that need to be distributed to the worker nodes, they can be combined in zip file, which can be passed as a `--py-files` argument.\n",
    "\n",
    "A sample simple application is provided in [projects/simple-pyspark-project](projects/simple-pyspark-project).  The main script is in [SimpleApp.py](projects/simple-pyspark-project/SimpleApp.py).  It imports a utility function from [myutils.py](projects/simple-pyspark-project/myutils.py), so it can be launched as so:\n",
    "\n",
    "```bash\n",
    "$ spark-submit --master local[*] --py-files myutils.py \\\n",
    "  SimpleApp.py <path on HDFS>\n",
    "```\n",
    "\n",
    "### Scala\n",
    "\n",
    "The equivalent Scala version of this sample application is in [projects/simple-spark-project](projects/simple-spark-project).  There is now an additional build step.\n",
    "\n",
    "1. **Build your Spark application**: Scala is a compiled language so you will need to build a jar that can be run on the Java Virtual Machine (JVM).  JAR (Java Archive) is a package file format typically used to aggregate many Java class files and associated metadata and resources (text, images, etc.) into one file to distribute application software or libraries on the Java platform.  Go to the project directory `projects/simple-spark-project` and run\n",
    "```bash\n",
    "$ sbt package\n",
    "```\n",
    "\n",
    "2. Submit the job locally running on 4 cores:\n",
    "```bash\n",
    "$ spark-submit \\\n",
    "     --class \"com.thedataincubator.simplespark.SimpleApp\" \\\n",
    "     --master local[4] \\\n",
    "     target/scala-2.10/simple-project_2.10-1.0.jar <path on HDFS>\n",
    "```\n",
    "\n",
    "You can use `local[*]` to run with as many worker threads as logical cores on your machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Packaging using sbt\n",
    "\n",
    "### What is `sbt`?\n",
    "`sbt` is a modern build tool written in and for Scala, though it is also a general purpose build tool.  `sbt` is actually a Scala [Domain Specific Language (DSL)](https://en.wikipedia.org/wiki/Domain-specific_language), meaning it's actually Scala (with enough new constructs to look like it's not Scala).  To invoke SBT, run\n",
    "``` bash\n",
    "$ sbt\n",
    "```\n",
    "which brings you into an \"sbt session.\"  The commands given below are to be typed within an SBT session.\n",
    "\n",
    "### Why `sbt`?\n",
    "- Sane(ish) dependency management\n",
    "- Incremental recompilation and keeping the compiler alive in between compilations (see [this article](http://www.scala-sbt.org/0.13.2/docs/Detailed-Topics/Understanding-incremental-recompilation.html))\n",
    "- Automatic recompilation triggered by file-change.  Within an sbt session, enter:\n",
    "    ```\n",
    "    ~compile\n",
    "    ```\n",
    "- Run the program within sbt:\n",
    "    ```\n",
    "    run\n",
    "    ```\n",
    "- Test the program within sbt:\n",
    "    ```\n",
    "    test\n",
    "    ```\n",
    "- Full Scala language support for creating tasks (it's a DSL)\n",
    "- Launch REPL in project context\n",
    "    ```\n",
    "    console # gives you a Scala repl within your jar\n",
    "    ```\n",
    "    and you can type commands into the REPL to play around\n",
    "    ```scala\n",
    "    import com.thedataincubator.simplespark.SimpleApp\n",
    "\n",
    "    val x = 1\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Project layout and directory structure\n",
    "\n",
    "A sample simple application is provided in [projects/simple-spark-project](projects/simple-spark-project).\n",
    "\n",
    "### Source files:\n",
    "1. `src/main` – your app code goes here, in a subdirectory indicating the code’s language, e.g.\n",
    "    1. `src/main/scala`\n",
    "    1. `src/main/java`\n",
    "1. `src/main/resources` – static files you want added to your jar (e.g. logging config)\n",
    "1. `src/test` – like src/main, but for tests  \n",
    "1. `src/main/scala/com/thedataincubator/simplespark/SimpleApp.scala` - an actual code file.  This is in two components:\n",
    "    1. `src/main/scala` - overhead (explained above)\n",
    "    1. `com/thedataincubator/simplespark/` - related to the package hierarchy (b/c it's written by people at the domain `thedataincubator.com`).  There are two files in our sample app:\n",
    "        1. `src/main/scala/com/thedataincubator/simplespark/SimpleApp.scala` - main app\n",
    "        1. `src/main/scala/com/thedataincubator/simplespark/Foo.scala` - helper class and methods\n",
    "    \n",
    "    This affects your code in two places:\n",
    "        1. All your `*.scala` files in this directory need to declare their packages consistently with their directory\n",
    "        ```scala\n",
    "        package com.thedataincubator.simplespark\n",
    "        ```\n",
    "        You can then easily access other files in this folder (package).  For example, there is also a `Foo.scala` in this directory (with the same package definition) and we can access it directly in `SimpleApp.scala` \n",
    "        1. When you invoke the jar, you need to specify the class (package) name (see the `spark-submit` command above)\n",
    "\n",
    "### Build files:\n",
    "1. `build.sbt` - This is like a make file.  It tells `sbt` how to build your project.  You can specify the version of your application, the Scala version you want, and the version of the dependencies you require (e.g. Spark) in the `build.sbt`:\n",
    "    ```scala\n",
    "    name := \"Simple Project\"\n",
    "    version := \"1.0\"\n",
    "    scalaVersion := \"2.10.4\"\n",
    "    libraryDependencies += \"org.apache.spark\" %% \"spark-core\" % \"1.2.0\"\n",
    "    ```\n",
    "1. `project/` – Because the `sbt` compiler is actually Scala code, the compiler has to be built.  It is built with `sbt`.  The instructions for how to build this meta-build are placed in this directory.  This allows you to tweak the build's build.\n",
    "1. `project/build.sbt` - The instructions for the meta-build (like `build.sbt` but for the compiler, not for your main project).  You can also tweak the build's build's build by having `project/project` and continue iterating forever (see [Organizing Build's](http://www.scala-sbt.org/0.13/tutorial/Organizing-Build.html)).\n",
    "\n",
    "### Output files:\n",
    "1. `target/` – The destination for generated files (e.g. class files, jars)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Further information:\n",
    "For more, check out the [documentation](http://www.scala-sbt.org/0.13/tutorial/Directories.html).\n",
    "\n",
    "### Alternatives:\n",
    "[Maven](https://maven.apache.org/) is another project management tool known for its reporting, documentation, and compatibility with continuous integration. SBT has become more popular as Scala becomes more ubiquitous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Spark on Amazon Web Services\n",
    "\n",
    "Amazon Web Services has extremely thorough documentation around everything from the commands available to the command line interface (CLI) `aws {commands}`, to the Python wrapper for said interface `boto`, to full tutorials and examples on how to fire up an EMR cluster or a bunch of EC2 instances with almost any desired data processing framework.\n",
    "\n",
    "EC2 is cheaper than EMR (Elastic Map Reduce), but EMR is recommended for immediate use of Hadoop and any other project in the ecosystem because it takes care of the Hadoop/YARN configuration for us. It's somewhat configurable via [Amazon Machine Images](http://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/ami-versions-supported.html) (AMIs). In a production setting it's possible you'll want to use specific versions for consistency; in our case it's safe to use the most recent version (`3.8.0` at the time of this writing).\n",
    "\n",
    "You can use the boto API in Python to programmatically check the status of jobs and clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Setting up a personal AWS account\n",
    "\n",
    "To use AWS you'll need to [create an account](http://aws.amazon.com/) if you haven't already. For the first year after new account creation, you'll be eligible for discounts on some services as part of the Free Tier program.\n",
    "\n",
    "Access the AWS [web console](https://console.aws.amazon.com/s3/) to handle most of your configuration. You'll need at least one S3 bucket to serve as storage for your logs and output. You'll also want to upload your actual code.\n",
    "\n",
    "From there you can create EMR clusters as you wish and run jobs. Be careful about the nodes you use, as only certain sizes are eligible for the free tier discounts. Still, you only pay for what you use, and the costs for small, educational jobs are relatively manageable.\n",
    "\n",
    "There's an in depth [tutorial](http://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/emr-get-started.html) available, and more detailed cluster configuration information can be found in this notebook, and in the Spark module.\n",
    "\n",
    "Also note that in addition to the normal credentials you might need to take care of:\n",
    "* Generating an EC2 keypair (this is separate from the AWS general keypair and goes in .mrjob.conf)\n",
    "* `aws emr create-default-roles` if you plan on just using the defaults for EMR\n",
    "* Making sure your user is part of a group with sufficient permissions (admin is probably fine)\n",
    "* You can set up multiple profiles in the `~/.aws/credentials` file in order to facilitate copying data from our S3 bucket while still being able to access your own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### AWS credentials and command line tools\n",
    "\n",
    "1. To verify that it is working, try \n",
    "``` bash\n",
    "aws s3 ls\n",
    "```\n",
    "You should get back a json blob that contains the permissions you just added for your user.  If not, double-check that you got your permissions setup correctly.\n",
    "\n",
    "1. `boto` ([docs](https://boto.readthedocs.org/en/latest/)) is a python library that wraps the functionality of `awscli`.  You can install it using\n",
    "``` bash\n",
    "pip install boto\n",
    "```\n",
    "and follow the instructions in the docs to get started.\n",
    "\n",
    "1. Another option for interacting with s3 from the command line is `s3cmd`. You can download/start using it via   \n",
    "``` bash\n",
    "git clone https://github.com/s3tools/s3cmd.git\n",
    "```\n",
    "and follow the documentation [here](https://github.com/s3tools/s3cmd).\n",
    "\n",
    "### S3 buckets\n",
    "\n",
    "You will need to create one (or more) AWS S3 buckets.  These will hold both the input data and the source code or JAR files for your application.  They will also provide a location for writing logs and output from your application.  Buckets can be managed from the [web console](https://console.aws.amazon.com/s3/home).\n",
    "\n",
    "For our purposes, we will assume that you've created the bucket `s3://my-bucket/`.  In here you will create directories `src/`, where you upload your code or JAR file, and `logs/`, where EMR will write logs from the cluster.  Often you will also create `data/` to store the input data, and `output/` to store the processed data.\n",
    "\n",
    "### Third party software\n",
    "\n",
    "There are some third-party tools that can help navigate AWS S3. It can be time-consuming to go through the command line looking for logs when there's no autocomplete or easily viewable directory structure - in which case something like Bucket Explorer might save you some time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Running jobs\n",
    "\n",
    "Before running a job, we need to create a cluster to run the job on.  This can be done via the [web console](https://console.aws.amazon.com/elasticmapreduce/home) or from the command line.  The following command will launch a cluster of three *m3.xlarge* machines.  One will act as the master node, while the other two will be slave nodes running the computation.  (This should be entered as a single line without the comments; we've added comments and removed line-continuation characters for clarity.)\n",
    "\n",
    "```bash\n",
    "aws emr create-cluster\n",
    "  --release-label emr-5.4.0        # Sets software versions\n",
    "  --name 'MySparkCluster'\n",
    "  --log-uri 's3://my-bucket/logs`\n",
    "  --use-default-roles\n",
    "  --ec2-attributes KeyName=my-key  # Name of SSH keypair\n",
    "  --applications Name=Spark        # Install Spark appropriately\n",
    "  --instance-type m3.xlarge\n",
    "  --instance-count 3\n",
    "```\n",
    "\n",
    "This will immediately return a cluster name, like *j-3VIY0CQC7QOFN*, but it will take several minutes for the cluster to come up fully.  You can watch the progress on the web console.  Jobs can be added to the cluster with the `add-steps` command:\n",
    "\n",
    "*Scala/Java*:\n",
    "```bash\n",
    "aws emr add-steps\n",
    "  --cluster-id j-3VIY0CQC7QOFN     # From creation step\n",
    "  --steps Name=SimpleApp,Type=spark,Args=[--deploy-mode,cluster,--master,yarn,--class,com.my-org.my-app.Main,s3://my-bucket/src/Application.jar,arg1,arg2,...],ActionOnFailure=CONTINUE\n",
    "```\n",
    "\n",
    "*Python*:\n",
    "```bash\n",
    "aws emr add-steps\n",
    "  --cluster-id j-3VIY0CQC7QOFN     # From creation step\n",
    "  --steps Name=SimpleApp,Type=spark,Args=[--deploy-mode,cluster,--master,yarn,--py-files,s3://my-bucket/src/utilites.py,s3://my-bucket/src/main.py,arg1,arg2,...],ActionOnFailure=CONTINUE\n",
    "```\n",
    "\n",
    "Note that the arguments in the `Args` section are exactly those passed to `spark-submit`.  Here, `arg1`, `arg2`, etc., represent the arguments to be passed to your application.  These will generally include both an S3 bucket for data input and an S3 bucket to store the output.\n",
    "\n",
    "The cluster will continue to run (and cost you money!) when the job is complete.  Don't forget to shut it down from the web console or with\n",
    "\n",
    "```bash\n",
    "aws emr terminate-clusters --cluster-id j-3VIY0CQC7QOFN\n",
    "```\n",
    "\n",
    "This work flow is handy, in that it allows you to examine the cluster to try to figure out why a job failed.  Once you have the kinks ironed out, it can be handy to launch a cluster, immediately run a job on it, and shut it back down.  This can be accomplished by passing the `--steps` argument to `create-cluster` along with the `--auto-terminate` flag.\n",
    "\n",
    "There are a number of other options to play with.  Generally you need more compute power on the worker nodes than on the master nodes.  This command will create a cluster of 1 *m3.xlarge* machine as master and 9 *m3.2xlarge* machines as slaves.  It will immediate run a particular JAR file and then shut down when finished.\n",
    "\n",
    "\n",
    "``` bash\n",
    "aws emr create-cluster\n",
    "  --release-label emr-5.4.0\n",
    "  --name 'MySparkCluster'\n",
    "  --log-uri 's3://my-bucket/logs`\n",
    "  --use-default-roles\n",
    "  --ec2-attributes KeyName=my-key\n",
    "  --applications Name=Spark\n",
    "  --instance-groups Name=Master,InstanceGroupType=MASTER,InstanceType=m3.xlarge,InstanceCount=1 Name=Core,InstanceGroupType=CORE,InstanceType=m3.2xlarge,InstanceCount=9\n",
    "  --auto-terminate\n",
    "  --steps Name=SparkApp,Type=spark,\n",
    "Args=[--deploy-mode,cluster,--master,yarn,--class,com.my-org.my-app.Main,s3://my-bucket/src/Application.jar,arg1,arg2,...],ActionOnFailure=CONTINUE\n",
    "```\n",
    "\n",
    "Note that, when using third-party libraries, it is preferable to install them at bootstrapping time. See the [documentation](http://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/emr-plan-bootstrap.html) for examples of how to write a script that will run at cluster creation.\n",
    "\n",
    "### Spark on EC2\n",
    "\n",
    "Spark comes with several built-in scripts to launch and manage clusters on Amazon EC2. Data input/output will be a little trickier than EMR but there are some scripts which can help with that too. Check out the [documentation](http://spark.apache.org/docs/1.6.2/ec2-scripts.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Spark on Google Cloud Platform\n",
    "\n",
    "Cloud Dataproc is GCP's analog to EMR: a managed Hadoop cluster environment that uses Google Compute Engine instances under the hood. There's a comprehensive 60-day free trial with $300 of credit (which should be more than enough for our purposes). Here are some step-by-step instructions for getting started:\n",
    "\n",
    "1. Probably the easiest way to interact with GCP is through the [Cloud SDK](https://cloud.google.com/sdk/#Quick_Start). Once you authenticate through your Google Account, you won't need any other credentials and most of the necessary command line tools will be there.\n",
    "1. Make buckets on Google Storage as necessary.\n",
    "1. Go into the API manager and enable both the GCE and Dataproc APIs.\n",
    "1. `gsutil cp` to upload data to your buckets. Useful flags are -r (recursive) and -m (parallel operation for many small tasks). eg. `gsutil -m cp -r data/stackoverflow gs://mybucket/data/stackoverflow/`\n",
    "1. Scala apps have the dependencies packaged in. If you're using Python, you'll need to write a bash script that runs on cluster initialization if you need third party libraries:\n",
    "``` bash\n",
    "#!/bin/bash\n",
    "apt-get install -y python-pip\n",
    "pip install toolz\n",
    "pip install lxml\n",
    "```\n",
    "Save this as init.sh and upload it to gs as well.\n",
    "1. You will likely want to disable dynamic executor allocation due to the way Dataproc does cluster management.\n",
    "1. Creating a cluster:\n",
    "``` bash\n",
    "gcloud dataproc clusters create cluster-1 --initialization-actions gs://mybucket/code/init.sh --zone us-east1-c --master-machine-type n1-standard-2 --master-boot-disk-size 50 --num-workers 3 --worker-machine-type n1-standard-2 --worker-boot-disk-size 50 --num-worker-local-ssds 1 --project google-project-123456\n",
    "```\n",
    "1. Submitting a job:\n",
    "``` bash\n",
    "gcloud dataproc jobs submit pyspark --cluster cluster-1 --py-files gs://mybucket/code/classes.py gs://mybucket/code/app.py gs://mybucket/data/input/ gs://mybucket/results/\n",
    "```\n",
    "1. Make sure to delete your cluster after the job is done:\n",
    "``` bash\n",
    "gcloud dataproc clusters delete cluster-1`\n",
    "```\n",
    "Most of this can also be done from the web console. There's a [good tutorial](https://cloud.google.com/dataproc/tutorials/spark-scala) for Spark with Scala as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Upgrading from the free trial\n",
    "\n",
    "The free trial limits you to 8 YARN cores, including the master node, which realistically means the biggest cluster you can use on dataproc has 3 worker instances with 2 nodes each.\n",
    "\n",
    "It is a significant time saver to upgrade to a paid account. You will keep your free trial credit (it expires when your trial would have expired), the only difference is **you will have to manually cancel your account** to avoid being billed after the trial period expires. Doing this is painless and will increase your quota to 24 YARN cores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## A digression: Plotting (in-memory) with Spark and Scala\n",
    "\n",
    "### Adding  external dependencies\n",
    "TL;DR use the `--packages` flag for external dependencies with Maven coordinates and `--jars` flag for local dependency jars for anything *not* in your `build.sbt`.\n",
    "\n",
    "- **spark-shell**: 3rd party (Maven) dependencies in spark-shell with Maven coordinates: \n",
    "```bash\n",
    "$ spark-shell --master {host} --packages \"{group}:{artifact}:{version}\" --jars oneJar.jar,anotherJar.jar\"\n",
    "```\n",
    "\n",
    "- **spark-submit**: Include 3rd party library jars to spark-submit. Note that packages with Maven coordinates should properly be in your `build.sbt` if you are using `spark-submit`:\n",
    "```bash\n",
    "$ spark-submit --master {host} --jars {someJar}.jar,{anotherJar}.jar --class MyApp path/to/MyApp.jar\n",
    "```\n",
    "\n",
    "\n",
    "### Example: plot values in spark-shell\n",
    "\n",
    "**N.B.** To get plots to appear from DigitalOcean on your local machine, you have to turn on X11 forwarding by ssh'ing into your box with `ssh -X`\n",
    "\n",
    "- To add dependencies in spark-shell with Maven coordinates: \n",
    "```bash\n",
    "$ spark-shell --master local[4] --packages \"{group}:{artifact}:{version}\"\n",
    "```\n",
    "\n",
    "\n",
    "1. Fire up the spark-shell:\n",
    "```bash\n",
    "$ spark-shell --master local[*] --packages \"org.scalanlp:breeze-viz_2.10:0.11.2\"\n",
    "```\n",
    "\n",
    "1. Create a plot that appears in a separate window\n",
    "\n",
    "```scala\n",
    "scala> val a = Array(0.3, 0.9, 1.4, 34.21, -3.4)\n",
    "a: Array[Double] = Array(0.3, 0.9, 1.4, 34.21, -3.4)\n",
    "\n",
    "scala> import breeze.plot._\n",
    "import breeze.plot._\n",
    "\n",
    "scala> val f = Figure()\n",
    "f: breeze.plot.Figure = breeze.plot.Figure@73543048\n",
    "\n",
    "scala> val p = f.subplot(0)\n",
    "p: breeze.plot.Plot = breeze.plot.Plot@2e549515\n",
    "\n",
    "scala> p += plot(a,a,'.')\n",
    "res4: breeze.plot.Plot = breeze.plot.Plot@2e549515\n",
    "\n",
    "scala> val g = breeze.stats.distributions.Gaussian(0,1)\n",
    "...\n",
    "scala> val p2 = f.subplot(1,1)\n",
    "...\n",
    "scala> p2 += hist(g.sample(100000),100)\n",
    "```\n",
    "\n",
    "### Example: plot a distribution\n",
    "\n",
    "Check out Cloudera's [`KernelDensity.estimate` implementation](https://github.com/sryza/aas/tree/master/ch09-risk/src/main/scala/com/cloudera/datascience/risk)\n",
    "\n",
    "```scala\n",
    "import breeze.plot._\n",
    "import com.cloudera.datascience.risk._\n",
    "\n",
    "object AnalysisClass {\n",
    "    ...\n",
    "  def plotDistribution(samples: Array[Double]): Figure = {\n",
    "    val min = samples.min\n",
    "    val max = samples.max\n",
    "    // Using toList before toArray avoids a Scala bug\n",
    "    val domain = Range.Double(min, max, (max - min) / 100).toList.toArray\n",
    "    val densities = KernelDensity.estimate(samples, domain)\n",
    "    val f = Figure()\n",
    "    val p = f.subplot(0)\n",
    "    p += plot(domain, densities)\n",
    "    p.ylabel = \"Density\"\n",
    "    f\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "In order to run this, include the *breeze* dependency in your `build.sbt`, and include the path to the `com.cloudera.datascience` package in the `--jars` flag of spark-submit.\n",
    "\n",
    "```scala\n",
    "libraryDependencies  ++= Seq(\n",
    "  \"org.scalanlp\" %% \"breeze\" % \"0.11.2\",\n",
    "  \"org.scalanlp\" %% \"breeze-natives\" % \"0.11.2\"\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "*Copyright &copy; 2015 The Data Incubator.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
