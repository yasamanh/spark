{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%ShowTypes on\n",
    "\n",
    "val sqlContext = new org.apache.spark.sql.SQLContext(sc)\n",
    "\n",
    "// For implicit transformation of RDDs to DataFrames\n",
    "import sqlContext.implicits._\n",
    "\n",
    "// For telling Spark to look in the local file system\n",
    "import java.io._\n",
    "def localpath(path: String): String = {\n",
    "    \"file://\" + new java.io.File(\".\").getCanonicalPath + \"/\" + path\n",
    "}\n",
    "\n",
    "// For timing expression evaluation\n",
    "def time[R](block: => R): R = {\n",
    "    val start: Long = System.nanoTime()\n",
    "    val result = block\n",
    "    val end: Long = System.nanoTime()\n",
    "    val duration: Double = (end - start) / 1000000000.0\n",
    "    println(\"Elapsed time: \" + duration + \"s\")\n",
    "    result\n",
    "}\n",
    "\n",
    "println(\"Using Spark version \" + sc.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark MLlib\n",
    "<!-- requirement: small_data/gutenberg -->\n",
    "*Official documentation [here](https://spark.apache.org/docs/latest/mllib-guide.html).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms\n",
    "\n",
    "Spark supports a number of machine-learning algorithms.\n",
    "\n",
    "- Classification and Regression\n",
    "    - SVM, linear regression\n",
    "    - SVR, logistic regression\n",
    "    - Naive Bayes\n",
    "    - Decision Trees\n",
    "    - Random Forests and Gradient-Boosted Trees\n",
    "- Clustering\n",
    "    - K-means (and streaming K-means)\n",
    "    - Gaussian Mixture Models\n",
    "    - Latent Dirichlet Allocation\n",
    "- Dimensionality Reduction\n",
    "    - SVD and PCA\n",
    "- It also has support for lower-level optimization primitives:\n",
    "    - Stochastic Gradient Descent\n",
    "    - Low-memory BFGS and L-BFGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallelized SGD\n",
    "\n",
    "For linear models like SVM, Linear Regression, and Logistic Regression, the cost function we're trying to optimize is essentially an average over the individual error term from each data point. This is particularly great for parallelization.  For example, in linear regression, recall that the gradient is\n",
    "\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial \\log(L(\\beta))}{\\partial \\beta} &= \\frac{\\partial}{\\partial \\beta} \\frac{1}{2}\\sum_j \\|y_j - X_{j \\cdot} \\cdot \\beta\\| \\\\\n",
    "&= \\frac{1}{2}\\sum_j \\frac{\\partial}{\\partial \\beta} \\|y_j - X_{j \\cdot} \\cdot \\beta\\| \\\\\n",
    "& = \\sum_j y_j - X_{j \\cdot} \\cdot \\beta \\\\\n",
    "& \\approx \\sum_{sample \\mbox{ } j} y_j - X_{j \\cdot} \\cdot \\beta\n",
    "\\end{align}$$\n",
    "\n",
    "The key *mathematical properties* we have used are:\n",
    "\n",
    "1. the error functions are the sum of error contributions of different training instances\n",
    "1. linearity of the derivative\n",
    "1. associativity of addition\n",
    "1. downsampling giving an unbiased estimator\n",
    "\n",
    "Since the last sum is over the different training instances and these are stored on different nodes, we can parallelize the computation of the gradient in SGD across multiple nodes.  Of course, we still need to maintain the running weight $\\beta$ that has to be present on every node (through a broadcast variable that is updated).  Notice that SVM, Linear Regression, and Logistic Regression all have error functions that are just sums over training instances so SGD can be used for all these algorithms.\n",
    "\n",
    "Spark's [implementation](http://spark.apache.org/docs/latest/mllib-optimization.html#stochastic-gradient-descent-sgd) uses a tunable minibatch size parameter to sample a percentage of the features RDD. For each iteration, the updated weights are broadcast to the executors, and the update is calculated for each data point and sent back to be aggregated.\n",
    "\n",
    "Parallelization handles increasing number of sampled data points m quite well since there are no interaction terms and each calculation is independent. Controlling how the algorithm iterates to convergence is also important, and can be done with parameters for the total iterations and step size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML vs. MLlib packages\n",
    "\n",
    "Confusingly, there are two machine learning APIs in Spark, the `mllib` package based on RDDs and the `ml` package based on DataFrames. For years these have been developed somewhat in parallel, resulting in duplication and asymmetry in functionality.\n",
    "\n",
    "With Spark 2.0+, `mllib` is in maintenance mode and will likely be deprecated in future in favor of the DataFrame-based API which more closely resembles libraries like Scikit-learn. Below is one example of the RDD-based API; the rest of the notebook will focus on DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.regression.LinearRegressionWithSGD \n",
    "import org.apache.spark.mllib.evaluation.RegressionMetrics\n",
    "import org.apache.spark.mllib.regression.LabeledPoint\n",
    "import org.apache.spark.mllib.linalg.{Vector, Vectors}\n",
    "\n",
    "// parameters\n",
    "val trainingIterations = 10\n",
    "val trainingFraction = .6\n",
    "\n",
    "// generate data\n",
    "val data = sc.parallelize(1 to 10000).map( _ =>\n",
    "    LabeledPoint(\n",
    "        math.random,\n",
    "        Vectors.dense(math.random, math.random, math.random)\n",
    "    )\n",
    ")\n",
    "\n",
    "// split the training and test sets\n",
    "val (training, test) = {\n",
    "    val splits = data.randomSplit(\n",
    "        Array(trainingFraction, 1.0 - trainingFraction),\n",
    "        seed = 42L\n",
    "    )\n",
    "    (splits(0).cache(), splits(1))\n",
    "}\n",
    "\n",
    "// train model\n",
    "val lrmodel = LinearRegressionWithSGD.train(training, trainingIterations)\n",
    "\n",
    "// get r2 score\n",
    "val r2 = {\n",
    "    val predictionAndLabels = test.map{\n",
    "        case LabeledPoint(label, features) => \n",
    "            (lrmodel.predict(features), label)\n",
    "    }\n",
    "    new RegressionMetrics(predictionAndLabels).r2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're interested in methods for introspecting some of these objects, the inline `<tab>` autocomplete can help. You can also use `getClass` and its sub-methods for learning more about things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test.getClass.getPackage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test.getClass.getMethods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lrmodel.getClass.getName"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Spark ML\n",
    "Spark ML implements the ideas of transformers, estimators, and pipelines by standardizing APIs across machine learning algorithms. This can streamline more complex workflows.\n",
    "\n",
    "The core functionality includes:\n",
    "* DataFrames - built off Spark SQL, can be created either directly or from RDDs as seen above\n",
    "* Transformers - algorithms that accept a DataFrame as input and return a DataFrame as output\n",
    "* Estimators - algorithms that accept a DataFrame as input and return a Transformer as output\n",
    "* Pipelines - chaining together Transformers and Estimators\n",
    "* Parameters - common API for specifying hyperparameters\n",
    "\n",
    "For example, a learning algorithm can be implemented as an Estimator which trains on a DataFrame of features and returns a Transformer which can output predictions based on a test DataFrame.\n",
    "\n",
    "Full documentation can be found [here](http://spark.apache.org/docs/latest/ml-guide.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.classification.{LogisticRegression, LogisticRegressionModel}\n",
    "import org.apache.spark.ml.feature.{HashingTF, Tokenizer}\n",
    "import org.apache.spark.sql.Row\n",
    "\n",
    "val reviews = Seq(\n",
    "    (\"Prose is well-written, but style is an impediment to learning. Should be called 'Reviewing Spark,' not 'Learning Spark'\", 0.0),\n",
    "    (\"Nice Headstart to Spark\", 1.0),\n",
    "    (\"Start here: Excellent reference for Spark\", 1.0),\n",
    "    (\"Insightful and so Spark-tastic!\", 1.0),\n",
    "    (\"Good intro but wordy and lacking details in areas\", 0.0),\n",
    "    (\"Best of the Books Currently Available\", 1.0),\n",
    "    (\"A good resource for people interested in learning Spark\", 1.0),\n",
    "    (\"Great Overview\", 1.0)\n",
    ")\n",
    "\n",
    "val test_reviews = Seq(\n",
    "    (\"A decent guided tour of Spark and its major components.\", 0.0),\n",
    "    (\"10/10 would buy again\", 1.0),\n",
    "    (\"it is simple to follow. well organized. straight ...\", 1.0),\n",
    "    (\"Just what you need to get started in Apache Spark.\", 1.0),\n",
    "    (\"Very good book for learning Spark\", 1.0)\n",
    ")\n",
    "\n",
    "val training = sqlContext.createDataFrame(reviews).toDF(\"title\", \"label\").cache()\n",
    "val test = sqlContext.createDataFrame(test_reviews).toDF(\"title\", \"label\")\n",
    "\n",
    "val tokenizer = (new Tokenizer()\n",
    "    .setInputCol(\"title\")\n",
    "    .setOutputCol(\"words\"))\n",
    "val hashingTF = (new HashingTF()\n",
    "    .setInputCol(tokenizer.getOutputCol)\n",
    "    .setOutputCol(\"features\"))\n",
    "val logreg = (new LogisticRegression()\n",
    "    .setMaxIter(10)\n",
    "    .setRegParam(0.01))\n",
    "\n",
    "val tokens = tokenizer.transform(training)\n",
    "val hashes = hashingTF.transform(tokens)\n",
    "val model = logreg.fit(hashes)\n",
    "\n",
    "// Make predictions on test documents\n",
    "val test_tokens = tokenizer.transform(test)\n",
    "val test_hashes = hashingTF.transform(test_tokens)\n",
    "\n",
    "{ model.transform(test_hashes)\n",
    "    .select(\"title\", \"label\", \"prediction\")\n",
    "    .collect()\n",
    "    .foreach { case Row(text: String, label: Double, prediction: Double) =>\n",
    "               println(s\"$text -> label = $label, prediction = $prediction\")\n",
    "             }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Note that if you use a Pipeline it won't have a coefficients attribute.\n",
    "model.coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Rewrite the above using a Pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation and grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}\n",
    "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val pipeline = new Pipeline().setStages(Array(tokenizer, hashingTF, logreg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "/** Saving and loading examples\n",
    "pipeline.write.overwrite().save(localpath(\"models/unfit_pipeline\"))\n",
    "model.write.overwrite().save(localpath(\"models/fitted_pipeline\"))\n",
    "val sameModel = PipelineModel.load(localpath(\"models/fitted_pipeline\"))\n",
    "*/\n",
    "println(\"Model loaded from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val paramGrid = (new ParamGridBuilder()\n",
    "    .addGrid(hashingTF.numFeatures, Array(10, 100, 1000))\n",
    "    .addGrid(logreg.regParam, Array(0.1, 0.01, 0.001))\n",
    "    .build())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val cv = (new CrossValidator()\n",
    "    .setEstimator(pipeline)\n",
    "    .setEvaluator(new BinaryClassificationEvaluator)\n",
    "    .setEstimatorParamMaps(paramGrid)\n",
    "    .setNumFolds(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note*: A more traditional validation set without folding is available in `TrainValidationSplit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val cvModel = cv.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "{ cvModel.transform(test)\n",
    "    .select(\"title\", \"label\", \"prediction\")\n",
    "    .collect()\n",
    "    .foreach { case Row(text: String, label: Double, prediction: Double) =>\n",
    "               println(s\"$text -> label = $label, prediction = $prediction\")\n",
    "             }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cvModel.bestModel.asInstanceOf[PipelineModel].stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "{ cvModel\n",
    "    .bestModel\n",
    "    .asInstanceOf[PipelineModel]\n",
    "    .stages(2)\n",
    "    .asInstanceOf[LogisticRegressionModel]\n",
    "    .coefficients\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example algorithm: Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.{Word2Vec, Word2VecModel}\n",
    "\n",
    "// val text = sc.parallelize(reviews + test_reviews).map((line, score) => (line.split(\" \"), score)).toDF(\"text\", \"score\")\n",
    "val gutenberg = sc.textFile(localpath(\"small_data/gutenberg/\")).map(line => (line.split(\" \"), 1)).toDF(\"text\", \"score\")\n",
    "val w2v = new Word2Vec().setInputCol(\"text\").setOutputCol(\"vectors\")\n",
    "val model = w2v.fit(gutenberg)\n",
    "val result = model.transform(gutenberg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val vectorDF = model.getVectors\n",
    "model.findSynonyms(\"woman\", 10).rdd.take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorDF.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we try to convert this DataFrame to RDD, we'll end up with an RDD of Row objects. It'll then be very difficult to cast the objects back to Vectors.\n",
    "\n",
    "This is where DataSets come in handy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try{\n",
    "    val vectorDS = vectorDF.as[(String, Vector)]\n",
    "} catch {\n",
    "    case e => e.printStackTrace\n",
    "} finally {\n",
    "    println(\"Huh?\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// This is just a taste of how fun it is to have two parallel, slightly different projects\n",
    "import org.apache.spark.ml.linalg.{Vector, Vectors}\n",
    "import org.apache.spark.ml.linalg.Vectors._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Can also define a case class and use that here\n",
    "val vectorDS = vectorDF.as[(String, org.apache.spark.ml.linalg.Vector)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Now we don't get an RDD of Rows\n",
    "val vectorRDD = vectorDS.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val king_vec = vectorRDD.lookup(\"king\")(0)\n",
    "val queen_vec = vectorRDD.lookup(\"queen\")(0)\n",
    "val man_vec = vectorRDD.lookup(\"man\")(0)\n",
    "val woman_vec = vectorRDD.lookup(\"woman\")(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "println(king_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: you cannot easily add these vectors! These are really meant to be used just for machine-learning algorithms. Convert these to [Breeze](https://github.com/scalanlp/breeze) vectors for your basic linear algebra manipulations. Like numpy, Breeze uses low-level libraries like BLAS and LAPACK under the hood. It is not as full ledged as numpy and type-safety can be a little annoying though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import breeze.linalg._\n",
    "\n",
    "val king = new DenseVector(king_vec.toArray)\n",
    "val queen = new DenseVector(queen_vec.toArray)\n",
    "val woman = new DenseVector(woman_vec.toArray)\n",
    "val man = new DenseVector(man_vec.toArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "println(squaredDistance(queen, king) + \"\\n\" +\n",
    "        squaredDistance(queen, woman) + \"\\n\" +\n",
    "        squaredDistance(queen, man) + \"\\n\" +\n",
    "        squaredDistance(queen, (king + man - woman)) + \"\\n\" +\n",
    "        squaredDistance(queen, (king - man + woman)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorDS.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Note that calling `select` will return Rows again. Be careful!\n",
    "val sampleVector: org.apache.spark.ml.linalg.Vector = vectorDS.take(1)(0)._2\n",
    "println(sampleVector.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.VectorSlicer\n",
    "\n",
    "val firstSlicer = (new VectorSlicer()\n",
    "    .setInputCol(\"vector\")\n",
    "    .setOutputCol(\"first\")\n",
    "    .setIndices(Array(0)))\n",
    "\n",
    "val lastSlicer = (new VectorSlicer()\n",
    "    .setInputCol(\"vector\")\n",
    "    .setOutputCol(\"last\")\n",
    "    .setIndices(Array(sampleVector.size - 1)))\n",
    "\n",
    "val medSlicer = (new VectorSlicer()\n",
    "    .setInputCol(\"vector\")\n",
    "    .setOutputCol(\"med\")\n",
    "    .setIndices((45 until 55).toArray))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val output = medSlicer.transform(\n",
    "    lastSlicer.transform(\n",
    "        firstSlicer.transform(vectorDS)\n",
    "    )\n",
    ")\n",
    "output.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "\n",
    "val assembler = (new VectorAssembler()\n",
    "    .setInputCols(Array(\"first\", \"last\", \"med\"))\n",
    "    .setOutputCol(\"features\"))\n",
    "\n",
    "val newOutput = assembler.transform(output)\n",
    "newOutput.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.Binarizer\n",
    "\n",
    "val binarizer = (new Binarizer()\n",
    "    .setThreshold(0.015)\n",
    "    .setInputCol(\"features\")\n",
    "    .setOutputCol(\"binFeatures\"))\n",
    "\n",
    "val binOutput = binarizer.transform(newOutput)\n",
    "binOutput.select(\"features\", \"binFeatures\").take(1)(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Use SVM to predict colon cancer from gene expressions\n",
    "You can start getting a feel for the MLlib operations by following the [Spark docs example](https://spark.apache.org/docs/1.3.0/mllib-linear-methods.html#linear-support-vector-machines-svms) on this dataset.\n",
    "\n",
    "#### About the data format: LibSVM\n",
    "MLlib conveniently provides a data loading method, `MLUtils.loadLibSVMFile()`, for the LibSVM format for which many other languages (R, Matlab, etc.) also have loading methods.  \n",
    "A dataset of *n* features will have one row per datum, with the label and values of each feature organized as follows:\n",
    ">{label} 1:{value} 2:{value} ... n:{value}\n",
    "\n",
    "Take these two datapoints with six features and labels of -1 and 1 respectively as an example:\n",
    ">-1.000000  1:2.080750 2:1.099070 3:0.927763 4:1.029080 5:-0.130763 6:1.265460  \n",
    "1.000000  1:1.109460 2:0.786453 3:0.445560 4:-0.146323 5:-0.996316 6:0.555759 \n",
    "\n",
    "#### About the colon-cancer dataset\n",
    "This dataset was introduced in the 1999 paper [Broad patterns of gene expression revealed by clustering analysis of tumor and normal colon tissues probed by oligonucleotide arrays.](http://www.pnas.org/content/96/12/6745.short)  \n",
    "\n",
    "Here's the abstract of the paper:  \n",
    "> *Oligonucleotide arrays can provide a broad picture of the state of the cell, by monitoring the expression level of thousands of genes at the same time. It is of interest to develop techniques for extracting useful information from the resulting data sets. Here we report the application of a two-way clustering method for analyzing a data set consisting of the expression patterns of different cell types. Gene expression in 40 tumor and 22 normal colon tissue samples was analyzed with an Affymetrix oligonucleotide array complementary to more than 6,500 human genes. An efficient two-way clustering algorithm was applied to both the genes and the tissues, revealing broad coherent patterns that suggest a high degree of organization underlying gene expression in these tissues. Coregulated families of genes clustered together, as demonstrated for the ribosomal proteins. Clustering also separated cancerous from noncancerous tissue and cell lines from in vivo tissues on the basis of subtle distributed patterns of genes even when expression of individual genes varied only slightly between the tissues. Two-way clustering thus may be of use both in classifying genes into functional groups and in classifying tissues based on gene expression.*\n",
    "\n",
    "There are 2000 features, 62 data points (40 tumor (label=0), 22 normal (label=1)), and 2 classes (labels) for the colon cancer dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exit Tickets\n",
    "1. When would you use `org.apache.spark.mllib.linalg.Vector` versus `breeze.linalg.DenseVector`?\n",
    "1. Why can SVM, Linear Regression, and Logistic Regression be parallelized?  How would you parallelize KMeans?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright &copy; 2015 The Data Incubator.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark - Scala",
   "language": "scala",
   "name": "spark_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
