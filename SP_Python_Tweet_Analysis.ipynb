{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "matplotlib.rcParams['savefig.dpi'] = 144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "matplotlib.rcParams['savefig.dpi'] = 2 * matplotlib.rcParams['savefig.dpi']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet mini case study\n",
    "<!-- requirement: small_data/tweets -->\n",
    "\n",
    "This sample code goes through the streaming tweets you collected in `Spark_Streaming.ipynb` and shows how to import the JSON data into RDDs and DataFrames, then do some rudimentary analysis.\n",
    "\n",
    "There are some sample tweets pre-loaded in `small_data/tweets/preloaded/` which are there to make sure this code works regardless of the previous notebook. You may delete those once you collect your own data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "from datetime import datetime\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "def localpath(path):\n",
    "    return 'file://' + str(os.path.abspath(os.path.curdir)) + '/' + path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc = SparkContext(\"local[*]\", \"demo\")\n",
    "print sc.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Creating an RDD from data on disk\n",
    "jsonRDD = sc.textFile(localpath(\"small_data/tweets/*/part*\"), minPartitions = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Experiment with changing the number of partitions. You can also use transformations like `repartition` or `coalesce`.\n",
    "print jsonRDD.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Open up the UI on port 4040 in another tab\n",
    "print jsonRDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "samples = jsonRDD.take(5)\n",
    "print type(samples[0])\n",
    "print samples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "json_sample = json.loads(samples[0])\n",
    "print type(json_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print json.dumps(json_sample, indent=4, sort_keys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print json_sample[\"text\"]\n",
    "print json_sample[\"createdAt\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at how we can access individual elements across the entire dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark SQL and DataFrames - a convenient abstraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating a DataFrame from data on disk, and registering it in the temporary Hive metastore\n",
    "raw_df = sqlContext.read.json(localpath(\"small_data/tweets/*/part-*\"))\n",
    "raw_df.registerTempTable(\"tweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_df.filter(raw_df[\"user\"][\"followersCount\"] > 50).select([\"text\", \"isFavorited\"]).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = sqlContext.sql(\"SELECT user.lang, COUNT(*) as cnt FROM tweets GROUP BY user.lang ORDER BY cnt DESC LIMIT 25\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more complicated operations, pre-defined or user-defined functions may be necessary. You can always drop down to the RDD level for more granular manipulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching and persistence - the key to Spark's speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "timestamps = jsonRDD.map(lambda x: json.loads(x)) \\\n",
    "                    .map(lambda x: (x, x[\"createdAt\"])) \\\n",
    "                    .mapValues(lambda x: datetime.strptime(x, \"%b %d, %Y %I:%M:%S %p\")) \\\n",
    "                    .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%timeit -r1 -n1\n",
    "print timestamps.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%timeit -r1 -n1\n",
    "print timestamps.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "timestamps.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note:* Many common transformations work across Spark: on DStreams, DataFrames, and RDDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "timestamps.filter(lambda x: x[1].minute == 35).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A bit easier to read\n",
    "timestamps.filter(lambda (blob, time): time.minute == 35).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def string_to_boolean_tuple(target, string):\n",
    "    if target in string:\n",
    "        return (1, 1)\n",
    "    else:\n",
    "        return (0, 1)\n",
    "\n",
    "plot_data = timestamps.map(lambda (key, value): (value, key)) \\\n",
    "                      .map(lambda (time, tweet): (time.second, string_to_boolean_tuple(\"RT\", tweet[\"text\"]))) \\\n",
    "                      .reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1])) \\\n",
    "                      .mapValues(lambda (rts, total): 1.0 * rts / total) \\\n",
    "                      .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note:* Scala makes helper functions like the above easier to write inline, which helps with code readability and succintness. The Python API has less freedom in this regard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print type(plot_data)\n",
    "print len(plot_data)\n",
    "print plot_data[0]\n",
    "x_data = [tup[0] for tup in plot_data]\n",
    "y_data = [tup[1] for tup in plot_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "matplotlib.pyplot.plot(x_data, y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Gather a larger sample of tweets using eg. the template in the Spark Streaming notebook, and apply the above analysis on longer time scales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Extension:* Perform the same analysis, but directly on the DStream. Note the difference in the Streaming UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright &copy; 2016 The Data Incubator.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
